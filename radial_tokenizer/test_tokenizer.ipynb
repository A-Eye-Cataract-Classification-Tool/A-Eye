{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c18113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RADIAL TOKENIZER (SIMULATED PROJECTION) TEST ======\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from radial_tokenizer import RadialTokenizer\n",
    "\n",
    "# ==== CONFIG ====\n",
    "IMG_PATH = \"C:/Users/denni/Downloads/test.png\"\n",
    "IMG_NAME = \"sample1\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "\n",
    "# Create directories for the new outputs\n",
    "TOK9_DIR = os.path.join(OUTPUT_DIR, \"tokens_9D\")\n",
    "TOK192_DIR = os.path.join(OUTPUT_DIR, \"tokens_192D_projected\") # For projected tokens\n",
    "WEIGHTS_DIR = os.path.join(OUTPUT_DIR, \"projection_weights_test\") # For test weights\n",
    "\n",
    "os.makedirs(TOK9_DIR, exist_ok=True)\n",
    "os.makedirs(TOK192_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# ==== LOAD IMAGE ====\n",
    "image = cv2.imread(IMG_PATH)\n",
    "if image is None:\n",
    "    raise FileNotFoundError(f\"Image not found at {IMG_PATH}\")\n",
    "\n",
    "image = cv2.resize(image, (128, 128))\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_norm = image_rgb / 255.0\n",
    "img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).float()\n",
    "print(f\"✅ Loaded Image Tensor Shape: {img_tensor.shape}\")\n",
    "\n",
    "# ==== 1. TEST RADIAL TOKENIZER (9D Output) ====\n",
    "tokenizer = RadialTokenizer()\n",
    "tokens_9d = tokenizer(img_tensor)  # Shape: [1, 4, 9]\n",
    "print(f\"✅ Raw Tokens Shape (9D): {tokens_9d.shape}\")\n",
    "\n",
    "# ==== 2. SIMULATE THE MODEL'S PROJECTION LAYER ====\n",
    "# This step mimics what ModifiedMobileViT does internally.\n",
    "# We do it here to get the 192D tokens needed for the positional encoding test.\n",
    "projection_layer = nn.Linear(9, 192)\n",
    "tokens_192d_projected = projection_layer(tokens_9d)\n",
    "print(f\"✅ Simulated Projected Tokens Shape (192D): {tokens_192d_projected.shape}\")\n",
    "\n",
    "# ==== 3. VISUALIZE RINGS ====\n",
    "# (This part remains the same)\n",
    "ring_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 165, 0)]\n",
    "image_with_rings = image_rgb.copy()\n",
    "for idx, (r0, r1) in enumerate(tokenizer.rings):\n",
    "    cv2.circle(image_with_rings, center=tokenizer.center, radius=r1, color=ring_colors[idx], thickness=1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image_with_rings)\n",
    "plt.title(\"Radial Tokenizer Rings Overlay\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ==== 4. SAVE ALL OUTPUTS FOR NEXT STEPS ====\n",
    "torch.save(tokens_9d.cpu(), os.path.join(TOK9_DIR, f\"{IMG_NAME}.pt\"))\n",
    "torch.save(tokens_192d_projected.cpu(), os.path.join(TOK192_DIR, f\"{IMG_NAME}.pt\"))\n",
    "torch.save(projection_layer.state_dict(), os.path.join(WEIGHTS_DIR, f\"{IMG_NAME}.pt\"))\n",
    "\n",
    "print(f\"💾 Saved 9D raw tokens to: {os.path.join(TOK9_DIR, f'{IMG_NAME}.pt')}\")\n",
    "print(f\"💾 Saved 192D projected tokens to: {os.path.join(TOK192_DIR, f'{IMG_NAME}.pt')}\")\n",
    "print(f\"💾 Saved SIMULATED projection weights to: {os.path.join(WEIGHTS_DIR, f'{IMG_NAME}.pt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RADIAL POSITIONAL ENCODING TEST ======\n",
    "import torch\n",
    "import os\n",
    "from radial_positional_encoding import RadialPositionEmbedding\n",
    "\n",
    "# ==== CONFIG ====\n",
    "IMG_NAME = \"sample1\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "TOK192_DIR = os.path.join(OUTPUT_DIR, \"tokens_192D_projected\") # Use the new directory\n",
    "ENCODED_DIR = os.path.join(OUTPUT_DIR, \"encoded_tokens\")\n",
    "os.makedirs(ENCODED_DIR, exist_ok=True)\n",
    "\n",
    "# ==== LOAD PROJECTED TOKENS ====\n",
    "# Load the tokens that were projected to 192D in the previous cell\n",
    "tokens_192d = torch.load(os.path.join(TOK192_DIR, f\"{IMG_NAME}.pt\"))\n",
    "print(f\"✅ Loaded projected 192D tokens with shape: {tokens_192d.shape}\")\n",
    "\n",
    "# ==== RUN POSITIONAL ENCODER ====\n",
    "# The encoder expects 192D tokens, which is what we are providing\n",
    "encoder = RadialPositionEmbedding(num_rings=4, embed_dim=192)\n",
    "encoded_tokens = encoder(tokens_192d)\n",
    "torch.save(encoded_tokens.cpu(), os.path.join(ENCODED_DIR, f\"encoded_{IMG_NAME}.pt\"))\n",
    "\n",
    "print(f\"✅ Encoded tokens shape: {encoded_tokens.shape}\")\n",
    "print(f\"💾 Saved encoded tokens to: {os.path.join(ENCODED_DIR, f'encoded_{IMG_NAME}.pt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a89ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== VALIDATE OUTPUT SHAPES, CONTENT & VISUALIZE ======\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "IMG_NAME = \"sample1\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "TOK9_DIR = os.path.join(OUTPUT_DIR, \"tokens_9D\")\n",
    "TOK192_DIR = os.path.join(OUTPUT_DIR, \"tokens_192D_projected\")\n",
    "ENCODED_DIR = os.path.join(OUTPUT_DIR, \"encoded_tokens\")\n",
    "WEIGHTS_DIR = os.path.join(OUTPUT_DIR, \"projection_weights_test\")\n",
    "\n",
    "# ==== LOAD ALL FILES ====\n",
    "tokens_9d = torch.load(os.path.join(TOK9_DIR, f\"{IMG_NAME}.pt\"))\n",
    "tokens_192d_projected = torch.load(os.path.join(TOK192_DIR, f\"{IMG_NAME}.pt\"))\n",
    "encoded_tokens = torch.load(os.path.join(ENCODED_DIR, f\"encoded_{IMG_NAME}.pt\"))\n",
    "projection_weights = torch.load(os.path.join(WEIGHTS_DIR, f\"{IMG_NAME}.pt\"))\n",
    "\n",
    "# ==== SHAPE & DTYPE VALIDATION ====\n",
    "assert tokens_9d.shape == (1, 4, 9), f\"❌ 9D token shape mismatch: {tokens_9d.shape}\"\n",
    "assert tokens_192d_projected.shape == (1, 4, 192), f\"❌ 192D projected token shape mismatch: {tokens_192d_projected.shape}\"\n",
    "assert encoded_tokens.shape == (1, 4, 192), f\"❌ Encoded token shape mismatch: {encoded_tokens.shape}\"\n",
    "print(\"✅ Shape and dtype checks passed.\")\n",
    "\n",
    "# ==== PRINT TOKEN CONTENT ====\n",
    "print(\"\\n=== RAW 9D TOKENS (Mean, Std, Median for each ring) ===\")\n",
    "df_9d = pd.DataFrame(tokens_9d.squeeze(0).detach().numpy())\n",
    "print(df_9d.to_string(index=True))\n",
    "\n",
    "print(\"\\n=== SIMULATED PROJECTION WEIGHTS (9 -> 192) ===\")\n",
    "for name, weights in projection_weights.items():\n",
    "    print(f\"\\nLayer: {name}, Shape: {weights.shape}\")\n",
    "\n",
    "# ==== VISUALIZE GRAPHS ====\n",
    "\n",
    "# Plot 1: Raw 9D Tokens\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(tokens_9d.squeeze(0).detach().numpy(), aspect='auto', cmap='viridis')\n",
    "plt.title(\"Raw 9D Tokens Per Ring\")\n",
    "plt.xlabel(\"Feature (Mean_R, G, B, Std_R, G, B, Median_R, G, B)\")\n",
    "plt.ylabel(\"Ring Index\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Projected 192D Tokens\n",
    "tokens_proj_np = tokens_192d_projected.squeeze(0).detach().numpy()\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    plt.plot(tokens_proj_np[i], label=f\"Ring {i+1}\")\n",
    "plt.title(\"192D Projected Tokens Per Ring\")\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Positionally Encoded 192D Tokens\n",
    "tokens_enc_np = encoded_tokens.squeeze(0).detach().numpy()\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    plt.plot(tokens_enc_np[i], label=f\"Ring {i+1}\")\n",
    "plt.title(\"192D Positionally Encoded Tokens Per Ring\")\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CELL 4: DISPLAY FULL CONTENT OF ALL SAVED .pt FILES ======\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Ensure these paths match the directories used in the previous cells\n",
    "IMG_NAME = \"sample1\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "TOK9_DIR = os.path.join(OUTPUT_DIR, \"tokens_9D\")\n",
    "TOK192_DIR = os.path.join(OUTPUT_DIR, \"tokens_192D_projected\")\n",
    "ENCODED_DIR = os.path.join(OUTPUT_DIR, \"encoded_tokens\")\n",
    "WEIGHTS_DIR = os.path.join(OUTPUT_DIR, \"projection_weights_test\")\n",
    "\n",
    "# Set pandas display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000) # Increased width for better formatting\n",
    "pd.set_option('display.precision', 6) # Display more decimal places\n",
    "\n",
    "# --- 1. LOAD AND DISPLAY 9D RAW TOKENS ---\n",
    "print(\"=\"*80)\n",
    "print(\"1. Contents of the 9D Raw Tokens file (tokens_9d.pt)\")\n",
    "print(\"=\"*80)\n",
    "tokens_9d = torch.load(os.path.join(TOK9_DIR, f\"{IMG_NAME}.pt\"))\n",
    "print(f\"Tensor Shape: {tokens_9d.shape}\\n\")\n",
    "df_9d = pd.DataFrame(tokens_9d.squeeze(0).detach().numpy())\n",
    "df_9d.columns = [f'Feat_{i}' for i in range(9)]\n",
    "df_9d.index = [f'Ring_{i+1}' for i in range(4)]\n",
    "print(\"Table of 9D Tokens (Rows are rings, Columns are features):\")\n",
    "print(df_9d)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# --- 2. LOAD AND DISPLAY PROJECTION WEIGHTS ---\n",
    "print(\"=\"*80)\n",
    "print(\"2. Contents of the Projection Weights file (projection_weights_test.pt)\")\n",
    "print(\"=\"*80)\n",
    "projection_weights = torch.load(os.path.join(WEIGHTS_DIR, f\"{IMG_NAME}.pt\"))\n",
    "for name, weights in projection_weights.items():\n",
    "    print(f\"Layer Name: '{name}'\")\n",
    "    print(f\"Tensor Shape: {weights.shape}\")\n",
    "    print(\"Tensor Values:\")\n",
    "    print(weights.detach().numpy())\n",
    "    print(\"-\" * 20)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# --- 3. LOAD AND DISPLAY 192D PROJECTED TOKENS ---\n",
    "print(\"=\"*80)\n",
    "print(\"3. Contents of the 192D Projected Tokens file (tokens_192d_projected.pt)\")\n",
    "print(\"=\"*80)\n",
    "tokens_192d_projected = torch.load(os.path.join(TOK192_DIR, f\"{IMG_NAME}.pt\"))\n",
    "print(f\"Tensor Shape: {tokens_192d_projected.shape}\\n\")\n",
    "df_192d = pd.DataFrame(tokens_192d_projected.squeeze(0).detach().numpy())\n",
    "df_192d.index = [f'Ring_{i+1}' for i in range(4)]\n",
    "print(\"Table of 192D Projected Tokens:\")\n",
    "# The .iloc slicing has been REMOVED to show all columns\n",
    "print(df_192d)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# --- 4. LOAD AND DISPLAY 192D ENCODED TOKENS ---\n",
    "print(\"=\"*80)\n",
    "print(\"4. Contents of the 192D Positionally Encoded Tokens file (encoded_{IMG_NAME}.pt)\")\n",
    "print(\"=\"*80)\n",
    "encoded_tokens = torch.load(os.path.join(ENCODED_DIR, f\"encoded_{IMG_NAME}.pt\"))\n",
    "print(f\"Tensor Shape: {encoded_tokens.shape}\\n\")\n",
    "df_enc = pd.DataFrame(encoded_tokens.squeeze(0).detach().numpy())\n",
    "df_enc.index = [f'Ring_{i+1}' for i in range(4)]\n",
    "print(\"Table of 192D Encoded Tokens:\")\n",
    "# The .iloc slicing has been REMOVED to show all columns\n",
    "print(df_enc)\n",
    "print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# === Load Data ===\n",
    "npy_data = np.load(\"radial_pos_enc.npy\")\n",
    "pt_data = torch.load(\"radial_pos_enc.pt\")\n",
    "\n",
    "# === Print Shapes ===\n",
    "print(\"NPY shape:\", npy_data.shape)\n",
    "print(\"PT shape:\", pt_data.shape)\n",
    "\n",
    "# === Display First Ring Vector from Batch 1 ===\n",
    "print(\"\\nNPY - Batch 1, Ring 1 vector (first 10 dims):\")\n",
    "print(npy_data[0, 0][:10])\n",
    "\n",
    "print(\"\\nPT - Batch 1, Ring 1 vector (first 10 dims):\")\n",
    "print(pt_data[0, 0][:10])\n",
    "\n",
    "# === Visualize All 4 Ring Embeddings ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "for ring in range(npy_data.shape[1]):\n",
    "    plt.plot(npy_data[0, ring], label=f\"Ring {ring+1}\")\n",
    "plt.title(\"Learnable Positional Embeddings (Batch 1)\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Display Full Ring Vectors in Table ===\n",
    "df = pd.DataFrame(npy_data[0], index=[f\"Ring {i+1}\" for i in range(4)])\n",
    "df.columns = [f\"D{i}\" for i in range(npy_data.shape[2])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Paths to the output files\n",
    "output_dir = \"output\"\n",
    "tokens_9d_path = os.path.join(output_dir, \"radial_tokens_9D.pt\")\n",
    "tokens_192d_path = os.path.join(output_dir, \"radial_tokens_192D.pt\")\n",
    "projection_weights_path = os.path.join(output_dir, \"radial_tokens_projection_weights.pt\")\n",
    "metadata_path = os.path.join(output_dir, \"radial_tokens_metadata.pt\")\n",
    "\n",
    "# Load tensors\n",
    "tokens_9d = torch.load(tokens_9d_path)\n",
    "tokens_192d = torch.load(tokens_192d_path)\n",
    "\n",
    "print(f\"‚úî Loaded: 9D ‚Üí {tokens_9d.shape}, 192D ‚Üí {tokens_192d.shape}\")\n",
    "\n",
    "tokens_9d_np = tokens_9d.squeeze(0).detach().numpy()\n",
    "tokens_192d_np = tokens_192d.squeeze(0).detach().numpy()\n",
    "\n",
    "# Display 9D features\n",
    "df_9d = pd.DataFrame(tokens_9d_np,\n",
    "                     columns=[f\"Feature_{i+1}\" for i in range(9)],\n",
    "                     index=[f\"Ring_{i+1}\" for i in range(4)])\n",
    "\n",
    "print(\"\\nüîπ 9D Feature Vectors (Mean, Std, Median per Ring):\")\n",
    "display(df_9d)\n",
    "\n",
    "# Display 192D projected embeddings\n",
    "df_192d = pd.DataFrame(tokens_192d_np,\n",
    "                       columns=[f\"Dim_{i+1}\" for i in range(192)],\n",
    "                       index=[f\"Ring_{i+1}\" for i in range(4)])\n",
    "\n",
    "print(\"\\nüîπ 192D Projected Embeddings:\")\n",
    "display(df_192d)\n",
    "\n",
    "# Visualize 192D embeddings\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    plt.plot(tokens_192d_np[i], label=f\"Ring {i+1}\")\n",
    "plt.title(\"192D Token Embeddings per Ring\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Load projection layer weights\n",
    "if os.path.exists(projection_weights_path):\n",
    "    projector_state = torch.load(projection_weights_path)\n",
    "    weight = projector_state['proj.weight'].detach().cpu().numpy()\n",
    "    bias = projector_state['proj.bias'].detach().cpu().numpy()\n",
    "\n",
    "    print(\"\\nüì¶ Projection Layer Weights [192 √ó 9]:\", weight.shape)\n",
    "    df_weights = pd.DataFrame(weight, columns=[f\"Input_{i+1}\" for i in range(9)])\n",
    "    display(df_weights.head())\n",
    "\n",
    "    print(\"\\nüì¶ Projection Layer Bias [192]:\", bias.shape)\n",
    "    print(\"Preview Bias Vector:\", bias[:10])\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No projection weight file found.\")\n",
    "\n",
    "# Load metadata\n",
    "if os.path.exists(metadata_path):\n",
    "    metadata = torch.load(metadata_path)\n",
    "    print(\"\\nüìå Metadata Contents:\")\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key}: Tensor ‚Üí {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No metadata file found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
